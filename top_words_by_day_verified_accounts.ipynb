{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b7565a",
   "metadata": {},
   "source": [
    "# Funcoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8dd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import unicodedata\n",
    "import sys\n",
    "nltk.download('stopwords')\n",
    "from datetime import datetime\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "\n",
    "def strip_accents(text):\n",
    "\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return str(text)\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "def remove_hashtag(text):\n",
    "    words = text.split()\n",
    "    for i in words:\n",
    "        if i.startswith('#'):\n",
    "            words.remove(i)\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_at(text):\n",
    "    words = text.split()\n",
    "    remove = []\n",
    "    for i in words:\n",
    "        if i.startswith('@'):\n",
    "            remove.append(i)\n",
    "    for r in remove:\n",
    "        words.remove(r)\n",
    "\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_link(text):\n",
    "    urls = re.findall(r'(https?://[^\\s]+)', text)\n",
    "    for u in urls:\n",
    "        text = text.replace(u,\"\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    a = []\n",
    "    content = []\n",
    "    words = txt.split()\n",
    "\n",
    "    stop = nltk.corpus.stopwords.words('portuguese')\n",
    "    no_stop =[w for w in words if w.lower().strip() not in stop]\n",
    "\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    for word in no_stop:\n",
    "        word = regex.sub('', word)\n",
    "        word = re.sub(\"\\d+\", \" \", word)\n",
    "        content.append(word)\n",
    "\n",
    "    clean = []\n",
    "\n",
    "    for word in content:\n",
    "        nfkd = unicodedata.normalize('NFKD',word)\n",
    "        palavra = u''.join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "        q = re.sub('[^a-zA-Z0-9 \\\\\\]',' ', palavra)\n",
    "        if len(q)<3:\n",
    "            continue\n",
    "        clean.append(q.lower().strip())\n",
    "\n",
    "    tokens = [t for t in clean if len(t)>2 and not t.isdigit()]\n",
    "    ct =' '.join(tokens)\n",
    "    return ct\n",
    "\n",
    "def clean(text):\n",
    "    docs = remove_at(text)\n",
    "    docs = remove_link(docs)\n",
    "    docs = remove_stopwords(docs)\n",
    "    docs = remove_hashtag(docs)\n",
    "    return docs\n",
    "\n",
    "def load(file):\n",
    "    words = []\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "    for l in lines:\n",
    "        words.append(l.replace(\"\\n\",\"\"))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c3969",
   "metadata": {},
   "source": [
    "# Extracting nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48dd7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref https://github.com/fmaruki/Nltk-Tagger-Portuguese\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "def topWords(df, number, useStop, stopFile):\n",
    "\n",
    "    selected = []\n",
    "    for line in df.text:\n",
    "        if any(ele in line for ele in words):\n",
    "            selected.append(clean(line))\n",
    "\n",
    "    stops = load(stopFile)\n",
    "\n",
    "    text = ' '.join(selected)\n",
    "\n",
    "    tagger = pickle.load(open(\"tagger.pkl\",'rb'))\n",
    "    portuguese_sent_tokenizer = nltk.data.load(\"tokenizers/punkt/portuguese.pickle\")\n",
    "    sentences = portuguese_sent_tokenizer.tokenize(text)\n",
    "    tags = [tagger.tag(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
    "    nouns = []\n",
    "    for t in tags[0]:\n",
    "        if t[1]=='NOUN':\n",
    "            if not t[0].startswith(\"kk\"):\n",
    "                nouns.append(t[0])\n",
    "\n",
    "    if useStop:\n",
    "        res = [i for i in nouns if i not in stops]\n",
    "    else:\n",
    "        res = nouns\n",
    "    commons = Counter(res).most_common(number)\n",
    "\n",
    "    return commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162a20ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "years = [\"20\", \"21\"]\n",
    "months = [\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"]\n",
    "path = \"my-path\"\n",
    "\n",
    "n = 100\n",
    "\n",
    "words = [\"corona\", \"covid\"]\n",
    "\n",
    "top_list = []\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        for day in range(1,32):\n",
    "            if day<10:\n",
    "                day = '0'+str(day)\n",
    "            filename = \"tweets_\"+str(day)+month+year+\".txt\"\n",
    "            \n",
    "            if os.path.isfile(path+filename):\n",
    "                df = pd.read_csv(path+filename, sep=\",\",header=0, quotechar=\"'\") \n",
    "                top = topWords(df[df[\"verified\"]==True], n, True, 'stopVerbs')\n",
    "                line = \"\"\n",
    "                for t in top:\n",
    "                    line += \",'\" + t[0] + \"',\" + str(t[1])\n",
    "                top_list.append(str(day) + month + year + line)\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6dd436",
   "metadata": {},
   "outputs": [],
   "source": [
    "topFilename = \"top\"+str(n)+\"words_verified.txt\"\n",
    "topWordsFile = open(topFilename, \"w\")\n",
    "fields = \"\"\n",
    "\n",
    "for i in range(1,n+1):\n",
    "    fields +=\",word\"+str(i)+\",count\"+str(i)\n",
    "\n",
    "topWordsFile.write(\"dateDDMMYY\" + fields+\"\\n\")\n",
    "\n",
    "topWordsFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "topWordsFile = open(topFilename, \"a\")\n",
    "\n",
    "for top in top_list:\n",
    "    topWordsFile.write(top.replace('\"','')+\"\\n\")\n",
    "topWordsFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(\"Finish: \")\n",
    "print(str(now))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
